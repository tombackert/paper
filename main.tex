%%%% ijcai24.tex

\typeout{IJCAI--24 Instructions for Authors}

% These are the instructions for authors for IJCAI-24.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai24.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai24}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}



%f체r symbols
\usepackage{ dsfont }
\usepackage{ amssymb }


% Comment out this line in the camera-ready submission
\linenumbers

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2024.0)
}

\title{White-Box vs Black-Box Methods: A Comparative Study on Explaining DNNs}


% Single author syntax
\author{
    Tom Backert
    \affiliations
    University of Luebeck, Luebeck, Germany\\
    Institut for Software Engineering und Programming Languages
    \emails
    tom.backert@student.uni-luebeck.de
}

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
\iffalse
\author{
First Author$^1$
\and
Second Author$^2$\and
Third Author$^{2,3}$\And
Fourth Author$^4$\\
\affiliations
$^1$First Affiliation\\
$^2$Second Affiliation\\
$^3$Third Affiliation\\
$^4$Fourth Affiliation\\
\emails
\{first, second\}@example.com,
third@other.example.com,
fourth@example.com
}
\fi

% Here starts the main part of the paper
\begin{document}

\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction}
\subsection{Motivation and Practical Goal}
As Artificial Intelligence (AI) models are getting more and more complex, it consequently gets harder for humans to understand these systems. In some areas in particular Deep Neural Networks (DNNs) models getting so complex that even expert are loosing sight making these models essentially a Black-Box. 

As AI models become increasingly integrated into our lives, it is crucial to understand the factors that influence their decisions. This has raised the need to have explanations for AI model decisions and played an important role in motivating researchers to focus on making complicated models explainable.

Explainable Artificial Intelligence (XAI) is a field of research that focuses on making AI models more interpretable and understandable to humans. XAI aims to bridge the gap between the black box nature of traditional AI models and the need for transparency and accountability in decision-making processes. 

The need for trust in decisions driven by AI models becomes particularly important in high-stakes domains such as healthcare and finance or autonomous systems such as self driving, where decisions can have significant impacts on people's lives.

XAI can help in improving AI models by identifying potential biases, errors, or areas for optimization. By understanding how models make predictions, we can better identify and address issues that may lead to inaccurate or unfair outcomes and may otherwise potentially cause real harm.

XAI can facilitate better collaboration between humans and AI systems. By providing explanations, AI models can become more effective partners in decision-making, allowing humans to understand the rationale behind the AI's suggestions and make informed choices.

XAI is an essential field for ensuring the responsible and safe use of AI systems. XAI holds its relevance in all areas of AI. As AI models continue to evolve, XAI techniques will play a crucial role in ensuring their transparency, accountability, and integration into society.

In this paper XAI is examined in the context of computer vision, in particular in the context of Convolutional Neural Networks (CNNs). 

\subsection{Problem Statement}
\subsection{Scientific Contributions}

\subsection{Focus}
The goal of this paper is to compare White-box and Black-Box methodes. To narrow the scope and compare more easily one examplary methode of each region was chosen. This paper specifically compares LIME ~\cite{ribeiro2016why} (Black-Box) and Grad-CAM ~\cite{Selvaraju_2019} (White-Box).


\section{Related Work}
\subsection{Context}
\subsection{Comparison}
\subsection{Differentiation}

\section{Background}

\subsection{Fundamentals of XAI}
XAI techniques can be broadly categorized into two main approaches: White-box and Black-box methods.

White-box methods rely on an understanding of the internal workings of the AI model to generate explanations. This typically involves analyzing the model's architecture and parameters, such as analyzing the weights and activations of specific layers to extract interpretable insights.

Black-box methods, on the other hand, do not require prior knowledge of the model's internal structure. Instead, they treat the model as a black box and focus on analyzing its input-output behavior to generate explanations. This approach is particularly useful for complex models such as DNNs, where understanding the internal workings is challenging or impractical.

The selection of an XAI method hinges on the specific AI model and application. White-box methods offer highly interpretable explanations by analyzing the model's internal workings. However, their applicability diminishes with complex models like DNNs commonly used in computer vision. Conversely, black-box methods can explain DNNs, but their explanations may be less transparent and require more data for training.

The choice of XAI methods depends on the specific model and application, and both white-box and black-box approaches have their own merits as shown in this paper.



%GRADCAM Background
\subsection{Grad-CAM: White-Box-Methodes}
In this paper Gradient-weighted Class Activation Mapping (Grad-CAM) ~\cite{Selvaraju_2019} is used exemplary to illustrate the concept of white-box methodes.
As mentioned, white-box methodes focus on the internal architecture of a model. 

% intro
Grad-CAM uses the idea that deep CNNs catch high-level visual connections. The higher one goes in convolutional layers in the networtk, the higher-level the visual correlation gets. Grad-CAM tipically uses the last convolutional layer because that is where the highest level of visual semantics is to be found.
In these layers neurons are trimmed on spatial associations that are class specific (e.g. they look for round or straigt shaped parts in a MNIST image to choose between round or straigt shaped numbers.) 

% Ziel erkl채ren
In order to obtain a class-discriminative localization map Grad-CAM uses the gradient information flowing into the last convolutional layer of the CNN. With that information it can then assign importance to each neuron for a given input class.

% Formel erkl채rt
To aquire the class-discriminative localization map 

\begin{equation}
    L_{Grad-CAM}^{c} \in \mathds{R}^{ u \times v}
\end{equation}

of width u and height v for any class c, one must first compute the gradient of the score for class c, \(y^c\), with respect to feature map activations \(A^k\) of a convolutional layer: \( \dfrac{ \partial y^c}{ \partial A^k} \). 

To gain the neuron importance weights \(\alpha^c_k\) the gradients backpropagating are gloabally average-pooled across the width and height dimensions (indexed by i and j): 

\begin{equation}
    \alpha^c_k = \dfrac{1}{Z}\sum_{i} \sum_{j} \dfrac{\partial y^c}{\partial A^k}
\end{equation}


This weight $\alpha^c_k$ represents a partial approximation of the deep network downstram from $A$ and captures the significance of feature map $k$ for a target class $c$. We apply a Rectified Linear Unit (ReLU) to the linear combination of maps because we are only interested in features that have a favorable contribution to the class. 
\begin{equation}
    L_{Grad-CAM}^{c} = ReLU ( \sum_{k} \alpha^c_k A^k )
\end{equation}


This produces a heatmap identical in dimensions to the convolutional feature map ($14 \times 14$ in the case of VGG ~\cite{simonyan2015deep} and AlexNet ~\cite{krizhevsky2012} )




% LIME background
\subsection{LIME: Black-Box-Methodes}
In this paper Local Interpretable Model-Agnostic Explanation (LIME) ~\cite{ribeiro2016why} is used exemplary to illustrate the concept of Black-Box methodes.

% Overarching Goal of Lime
The primary goal of LIME is to provide explanations that are faithful to the predictions of the black-box model within a local region around the instance being explained. The objective is to provide an interpretable explanation that is understandable to humans regardless of the features utilized by the model. 
While LIME is adaptable for a variety of model families, this paper concentrates on it's implementation within computer vision. 
When representing images, an interpretable depiction might involve a binary vector indicating the presence or absense of superpixels (clusters of similar pixels).

% Detailed Goal: Lime in a nutshell
The goal is to approximate the model's intricate decision function $f$, which cannot be adequately captured by a linear model. To provide explanations for specific instances, LIME employs instance sampling (Pertubation) to obtain predictions from $f$, weighting them based on their proximity to the instance under examination. As a result, LIME produces explanations that are locally accurate but may not reflect global behavior faithfully.

% Fidelity-Interpretablility Trad-off
The explanation constructed by LIME is obtained by

\begin{equation}
\xi(x) = \arg \min_{g \in G}  \mathcal{L}(f, g, \pi_x) + \Omega(g)    
\end{equation}

A model $g \in G$ produced by LIME where $\Omega(g)$ denotes the measure of complexity of the explanation (for linear models $\Omega(g)$ the number of non-zero weights). 
$f(x)$ represents the probability that $x$ belongs to a specific class.
%grundprinzip
We employ $\pi_x(z)$ as a measure between an instance $z$ and $x$, shaping the local environment centered on the original instance. 
$x$ being the original representaion of an instance we want to explain. And $z$ being a perturbed instance (original instance with added noise) produced for the LIME explanation. 

%weitere erkl채rung
The locality-arware loss $\mathcal{L}(f, g, \pi_x)$ evaluates how accurately $g$ captures the essence of $f$ within the confines of $\Omega(g)$. 
To achieve both understandability and fidelity to local features requires minimizing $\mathcal{L}(f, g, \pi_x)$ without making any assumptions about $f$ while constraining $\Omega(g)$ to a level that supports interpretability. 

%Sampling for local exploration
Perturbed samples are generated by randomly selecting nonzero elements from a point and converting them into a new representation. These samples, along with their labels obtained from the original model, are gathered in a dataset $Z$. This is then used to train an explanation model (optimizing equation 1) to get an explanation $\xi(x)$. 

%Sparse Linear Explainations
We use the locally weighted square loss:

\begin{equation}
\mathcal{L}(f, g, \pi_x) = \sum_{z, z' \in Z } \pi_x(z)(f(z)-g(z'))^2
\end{equation}

where $\pi_x$ is an exponential kernel defined on the $L2$ Distance. We then approximate Eg.(1) by first selecting $K$ features with LASSO and then learning the weigths via least squares with K-LASSO.
%locally weighted square loss
The locally weighted square loss enables the explanation model to approximate the behavior of the black-box model within this local vicinity.
By using a locally weighted loss function, the explanation model is trained to focus more on instances that are closer to the instance being explained, thus capturing the local decision boundary of the black-box model more effectively.
The use of a distance-based kernel function (such as the exponential kernel mentioned in the paper) allows the loss function to emphasize instances that are more similar to the instance being explained while downweighting those that are farther away. This enables the explanation model to adapt to the local complexity of the black-box model, capturing intricate relationships within the local neighborhood. 


\subsection{Evaluation of Feature Importance Methods}
The challenge of comparing explanation techniques and objectively evaluating their quality lies in the nature of DNN predictions. Often, these predictions might only be interpretable by experts. Consequently, an explanation technique itself might also require expert knowledge for interpretation. To address this, we can introduce quantitative metrics.

Explanation Selectivity~\cite{MONTAVON20181} is a quantitative metric used to assess the quality of an explanation method for feature importance in DNNs. It essentially measures how well the explanation method identifies the features that have the strongest impact on the DNN's prediction.

The method works by iteratively removing features based on their assigned relevance scores (provided by the explanation method) and tracking how much the DNN's prediction value (function value) drops after removing each feature. A sharp drop in prediction value after removing a feature indicates high selectivity, meaning the explanation method effectively identified a feature with a strong influence on the prediction.

Explanation Selectivity is straightforward and relatively easy to apply. It offers intuitive interpretation: a lower AUC score indicates better selectivity. Furthermore, Explanation Selectivity is widely used and allows us to compare different explanation methods to see which one assigns relevance scores that best reflect the actual feature importance for the DNN's prediction. In this paper, Explanation Selectivity is used to compare the feature importance of LIME and Grad-CAM.


\section{Use Case Study}
\subsection{Relevance of Use Case}
\subsection{Supporting Decision Making}
\subsection{Building Trust}
\subsection{Experimental Setup}
\subsection{Experimental Results}

\section{Discussion}
\subsection{Interpretaion of Results}
\subsection{Application-oriented Evaluation}
\subsection{Combination}
\subsection{Generalization of Results}

\section{Conclusion}
\subsection{Summary of the key findings}
\subsection{Limitations}
\subsection{Further Work}


%% The file named.bst is a bibliography style file for BibTeX 0.99c

\bibliographystyle{named}
\bibliography{ijcai24}

\appendix

\end{document}