%%%% ijcai24.tex

\typeout{IJCAI--24 Instructions for Authors}

% These are the instructions for authors for IJCAI-24.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai24.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai24}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}

% Comment out this line in the camera-ready submission
\linenumbers

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2024.0)
}

\title{White-Box vs Black-Box Methods: A Comparative Study on Explaining DNNs}


% Single author syntax
\author{
    Tom Backert
    \affiliations
    University of Luebeck, Luebeck, Germany\\
    Institut for Software Engineering und Programming Languages
    \emails
    tom.backert@student.uni-luebeck.de
}

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
\iffalse
\author{
First Author$^1$
\and
Second Author$^2$\and
Third Author$^{2,3}$\And
Fourth Author$^4$\\
\affiliations
$^1$First Affiliation\\
$^2$Second Affiliation\\
$^3$Third Affiliation\\
$^4$Fourth Affiliation\\
\emails
\{first, second\}@example.com,
third@other.example.com,
fourth@example.com
}
\fi

% Here starts the main part of the paper
\begin{document}

\maketitle

\begin{abstract}
    
\end{abstract}

\section{Introduction}
\subsection{Motivation and Practical Goal}
As Artificial Intelligence (AI) models are getting more and more complex, it consequently gets harder for humans to understand these systems. In some areas in particular Deep Neural Networks (DNNs) models getting so complex that even expert loosing sight making thes models essentially a Black-Box. 

As AI models become increasingly integrated into our lives, it is crucial to understand the factors that influence their  decisions. This has raised the need to have explanations for AI model decisions and played an important role in motivating researchers to focus on making complicated models explainable.

Explainable Artificial Intelligence (XAI) is a field of research that focuses on making AI models more interpretable and understandable to humans.  XAI aims to bridge the gap between the black box nature of traditional AI models and the need for transparency and accountability in decision-making processes. 

The need for trust in decisions driven by AI models becomes particularly important in high-stakes domains such as healthcare and finance or autonomous systems such as self driving, where decisions can have significant impacts on people's lives.

XAI can help in improving AI models by identifying potential biases, errors, or areas for optimization. By understanding how models make predictions, we can better identify and address issues that may lead to inaccurate or unfair outcomes and may otherwise potentially cause real harm.

XAI can facilitate better collaboration between humans and AI systems. By providing explanations, AI models can become more effective partners in decision-making, allowing humans to understand the rationale behind the AI's suggestions and make informed choices.

XAI is an essential field for ensuring the responsible and safe use of AI systems. XAI holds its relevance in all areas of AI. As AI models continue to evolve, XAI techniques will play a crucial role in ensuring their transparency, accountability, and integration into society.

Though this paper only focuses on the context of computer vision,


\subsection{Problem Statement}
\subsection{Scientific Contributions}

\subsection{Focus}
The goal of this paper is to compare White-box and Black-Box methodes. To narrow the scope and compare more easily one examplary methode of each region was chosen. This paper specifically compares LIME ~\cite{ribeiro2016why} (Black-Box) and Grad-CAM ~\cite{Selvaraju_2019} (White-Box).


\section{Related Work}
\subsection{Context}
\subsection{Comparison}
\subsection{Differentiation}

\section{Background}

\subsection{Fundamentals of XAI}
XAI techniques can be broadly categorized into two main approaches: white-box and black-box methods.

White-box methods rely on an understanding of the internal workings of the AI model to generate explanations. This typically involves analyzing the model's architecture and parameters, such as analyzing the weights and activations of specific layers to extract interpretable insights.

Black-box methods, on the other hand, do not require prior knowledge of the model's internal structure. Instead, they treat the model as a black box and focus on analyzing its input-output behavior to generate explanations. This approach is particularly useful for complex models such as DNNs, where understanding the internal workings is challenging or impractical.

The selection of an XAI method hinges on the specific AI model and application. White-box methods offer highly interpretable explanations by analyzing the model's internal workings. However, their applicability diminishes with complex models like DNNs commonly used in computer vision. Conversely, black-box methods can explain DNNs, but their explanations may be less transparent and require more data for training.

The choice of XAI methods depends on the specific model and application, and both white-box and black-box approaches have their own merits.


\subsection{White-Box-Methodes}
In this paper Grad-CAM ~\cite{Selvaraju_2019} is used exemplary to illustrate the concept of White-Box methodes.



\subsection{Black-Box-Methodes}
In this paper Local Interpretable Model-Agnostic Explanation (LIME) ~\cite{ribeiro2016why} is used exemplary to illustrate the concept of Black-Box methodes.


\subsection{Evaluation of Feature Importance Methods}

The challenge of comparing explanation techniques and objectively evaluating their quality lies in the nature of DNN predictions. Often, these predictions might only be interpretable by experts. Consequently, an explanation technique itself might also require expert knowledge for interpretation. To address this, we can introduce quantitative metrics.

Explanation Selectivity~\cite{MONTAVON20181} is a quantitative metric used to assess the quality of an explanation method for feature importance in DNNs. It essentially measures how well the explanation method identifies the features that have the strongest impact on the DNN's prediction.

The method works by iteratively removing features based on their assigned relevance scores (provided by the explanation method) and tracking how much the DNN's prediction value (function value) drops after removing each feature. A sharp drop in prediction value after removing a feature indicates high selectivity, meaning the explanation method effectively identified a feature with a strong influence on the prediction.

Explanation Selectivity is straightforward and relatively easy to apply. It offers intuitive interpretation: a lower AUC score indicates better selectivity. Additionally, Explanation Selectivity is widely used and allows us to compare different explanation methods to see which one assigns relevance scores that best reflect the actual feature importance for the DNN's prediction. In this paper, Explanation Selectivity is used to compare the different methods LIME and Grad-CAM.


\section{Use Case Study}
\subsection{Relevance of Use Case}
\subsection{Supporting Decision Making}
\subsection{Building Trust}
\subsection{Experimental Setup}
\subsection{Experimental Results}

\section{Discussion}
\subsection{Interpretaion of Results}
\subsection{Application-oriented Evaluation}
\subsection{Combination}
\subsection{Generalization of Results}

\section{Conclusion}
\subsection{Summary of the key findings}
\subsection{Limitations}
\subsection{Further Work}





%% The file named.bst is a bibliography style file for BibTeX 0.99c

\bibliographystyle{named}
\bibliography{ijcai24}

\appendix

\end{document}